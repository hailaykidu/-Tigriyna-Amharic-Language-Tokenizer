GeezScriptTokenizer: is a language-specific tokenizer developed to handle the unique characteristics of Geez script languages, particularly Amharic and Tigrinya. This tokenizer is designed to effectively manage the complexities of these languages by accurately identifying and processing prefixes, postfixes, and word boundaries within the text. By incorporating these language-specific rules, GeezScriptTokenizer significantly improves tokenization efficiency, ensuring better representation and performance for tasks involving Amharic and Tigrinya.

This tokenizer is highly suited for natural language processing (NLP) tasks where standard multilingual tokenizers may struggle with the nuances of Geez script languages. Hailay/GeezScriptTokenizer is an ideal tool for researchers and developers working with these languages, providing a tailored approach to tokenization that enhances the overall quality of language models and downstream tasks./
scaling-with-vocab-trained-tokenizers for Geez Script languages such as Amharic and Tigriyna 

Details about the trained Models:  https://huggingface.co/Hailay/GeezScriptTokenizer 
